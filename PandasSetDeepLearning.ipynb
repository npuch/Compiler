{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PandasSetDeepLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1aWQ6n8w-kuGYDJKP26hxYy-Ie06qYAy9",
      "authorship_tag": "ABX9TyPWjZSd7bXk3BfftFdcZNEC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/npuch/Compiler/blob/master/PandasSetDeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC0DEI-6yilM"
      },
      "source": [
        "Mount Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDnKvaXQlTSE",
        "outputId": "c35770a7-e504-4493-e4ad-2a2a299012e4"
      },
      "source": [
        "  from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxjvDbEPKuQM"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBhFo2WvynMc"
      },
      "source": [
        "Kaggle -> Google Drive -> Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lTWLNhDmzGHy",
        "outputId": "c510b358-a325-4d4c-fec1-abeda89dc483"
      },
      "source": [
        "!git clone https://github.com/scaleapi/pandaset-devkit.git\n",
        "!pip install pandaset-devkit/python/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pandaset-devkit'...\n",
            "remote: Enumerating objects: 191, done.\u001b[K\n",
            "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 191 (delta 47), reused 22 (delta 7), pack-reused 96\u001b[K\n",
            "Receiving objects: 100% (191/191), 21.05 MiB | 33.01 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "Processing ./pandaset-devkit/python\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Collecting appnope==0.1.0\n",
            "  Downloading appnope-0.1.0-py2.py3-none-any.whl (4.0 kB)\n",
            "Collecting attrs==19.3.0\n",
            "  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
            "Collecting backcall==0.1.0\n",
            "  Downloading backcall-0.1.0.zip (11 kB)\n",
            "Collecting bleach==3.1.4\n",
            "  Downloading bleach-3.1.4-py2.py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting certifi==2019.11.28\n",
            "  Downloading certifi-2019.11.28-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 35.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from pandaset==0.2.dev0) (3.0.4)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from pandaset==0.2.dev0) (4.4.2)\n",
            "Collecting defusedxml==0.6.0\n",
            "  Downloading defusedxml-0.6.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: entrypoints==0.3 in /usr/local/lib/python3.7/dist-packages (from pandaset==0.2.dev0) (0.3)\n",
            "Collecting gmplot==1.2.0\n",
            "  Downloading gmplot-1.2.0.tar.gz (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 43.5 MB/s \n",
            "\u001b[?25hCollecting idna==2.9\n",
            "  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.1 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata==1.5.2\n",
            "  Downloading importlib_metadata-1.5.2-py2.py3-none-any.whl (30 kB)\n",
            "Collecting ipykernel==5.2.0\n",
            "  Downloading ipykernel-5.2.0-py3-none-any.whl (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 47.5 MB/s \n",
            "\u001b[?25hCollecting ipython==7.13.0\n",
            "  Downloading ipython-7.13.0-py3-none-any.whl (780 kB)\n",
            "\u001b[K     |████████████████████████████████| 780 kB 42.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.7/dist-packages (from pandaset==0.2.dev0) (0.2.0)\n",
            "Collecting ipywidgets==7.5.1\n",
            "  Downloading ipywidgets-7.5.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 51.2 MB/s \n",
            "\u001b[?25hCollecting jedi==0.16.0\n",
            "  Downloading jedi-0.16.0-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 44.4 MB/s \n",
            "\u001b[?25hCollecting Jinja2==2.11.1\n",
            "  Downloading Jinja2-2.11.1-py2.py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 60.9 MB/s \n",
            "\u001b[?25hCollecting jsonschema==3.2.0\n",
            "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.7/dist-packages (from pandaset==0.2.dev0) (1.0.0)\n",
            "Collecting jupyter-client==6.1.2\n",
            "  Downloading jupyter_client-6.1.2-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 51.4 MB/s \n",
            "\u001b[?25hCollecting jupyter-console==6.1.0\n",
            "  Downloading jupyter_console-6.1.0-py2.py3-none-any.whl (21 kB)\n",
            "Collecting jupyter-core==4.6.3\n",
            "  Downloading jupyter_core-4.6.3-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting MarkupSafe==1.1.1\n",
            "  Downloading MarkupSafe-1.1.1-cp37-cp37m-manylinux2010_x86_64.whl (33 kB)\n",
            "Requirement already satisfied: mistune==0.8.4 in /usr/local/lib/python3.7/dist-packages (from pandaset==0.2.dev0) (0.8.4)\n",
            "Requirement already satisfied: nbconvert==5.6.1 in /usr/local/lib/python3.7/dist-packages (from pandaset==0.2.dev0) (5.6.1)\n",
            "Collecting nbformat==5.0.4\n",
            "  Downloading nbformat-5.0.4-py3-none-any.whl (169 kB)\n",
            "\u001b[K     |████████████████████████████████| 169 kB 49.5 MB/s \n",
            "\u001b[?25hCollecting notebook==6.0.3\n",
            "  Downloading notebook-6.0.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.7 MB 20.2 MB/s \n",
            "\u001b[?25hCollecting numpy==1.18.2\n",
            "  Downloading numpy-1.18.2-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2 MB 9.6 MB/s \n",
            "\u001b[?25hCollecting pandas==1.0.3\n",
            "  Downloading pandas-1.0.3-cp37-cp37m-manylinux1_x86_64.whl (10.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.0 MB 49.0 MB/s \n",
            "\u001b[?25hCollecting pandocfilters==1.4.2\n",
            "  Downloading pandocfilters-1.4.2.tar.gz (14 kB)\n",
            "Collecting parso==0.6.2\n",
            "  Downloading parso-0.6.2-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pexpect==4.8.0 in /usr/local/lib/python3.7/dist-packages (from pandaset==0.2.dev0) (4.8.0)\n",
            "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.7/dist-packages (from pandaset==0.2.dev0) (0.7.5)\n",
            "Collecting Pillow==7.0.0\n",
            "  Downloading Pillow-7.0.0-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 35.6 MB/s \n",
            "\u001b[?25hCollecting prometheus-client==0.7.1\n",
            "  Downloading prometheus_client-0.7.1.tar.gz (38 kB)\n",
            "Collecting prompt-toolkit==3.0.5\n",
            "  Downloading prompt_toolkit-3.0.5-py3-none-any.whl (351 kB)\n",
            "\u001b[K     |████████████████████████████████| 351 kB 39.7 MB/s \n",
            "\u001b[?25hCollecting ptyprocess==0.6.0\n",
            "  Downloading ptyprocess-0.6.0-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: Pygments==2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandaset==0.2.dev0) (2.6.1)\n",
            "Collecting pyrsistent==0.16.0\n",
            "  Downloading pyrsistent-0.16.0.tar.gz (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 45.8 MB/s \n",
            "\u001b[?25hCollecting python-dateutil==2.8.1\n",
            "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 52.7 MB/s \n",
            "\u001b[?25hCollecting pytz==2019.3\n",
            "  Downloading pytz-2019.3-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[K     |████████████████████████████████| 509 kB 40.5 MB/s \n",
            "\u001b[?25hCollecting pyzmq==19.0.0\n",
            "  Downloading pyzmq-19.0.0-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 44.7 MB/s \n",
            "\u001b[?25hCollecting qtconsole==4.7.2\n",
            "  Downloading qtconsole-4.7.2-py2.py3-none-any.whl (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 45.9 MB/s \n",
            "\u001b[?25hCollecting QtPy==1.9.0\n",
            "  Downloading QtPy-1.9.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (from pandaset==0.2.dev0) (2.23.0)\n",
            "Collecting Send2Trash==1.5.0\n",
            "  Downloading Send2Trash-1.5.0-py3-none-any.whl (12 kB)\n",
            "Collecting six==1.14.0\n",
            "  Downloading six-1.14.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting terminado==0.8.3\n",
            "  Downloading terminado-0.8.3-py2.py3-none-any.whl (33 kB)\n",
            "Collecting testpath==0.4.4\n",
            "  Downloading testpath-0.4.4-py2.py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 48.6 MB/s \n",
            "\u001b[?25hCollecting tornado==6.0.4\n",
            "  Downloading tornado-6.0.4.tar.gz (496 kB)\n",
            "\u001b[K     |████████████████████████████████| 496 kB 39.8 MB/s \n",
            "\u001b[?25hCollecting traitlets==4.3.3\n",
            "  Downloading traitlets-4.3.3-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting urllib3==1.25.8\n",
            "  Downloading urllib3-1.25.8-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 60.4 MB/s \n",
            "\u001b[?25hCollecting wcwidth==0.1.9\n",
            "  Downloading wcwidth-0.1.9-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.7/dist-packages (from pandaset==0.2.dev0) (0.5.1)\n",
            "Collecting widgetsnbextension==3.5.1\n",
            "  Downloading widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 20.7 MB/s \n",
            "\u001b[?25hCollecting zipp==3.1.0\n",
            "  Downloading zipp-3.1.0-py3-none-any.whl (4.9 kB)\n",
            "Collecting transforms3d==0.3.1\n",
            "  Downloading transforms3d-0.3.1.tar.gz (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython==7.13.0->pandaset==0.2.dev0) (57.4.0)\n",
            "Building wheels for collected packages: pandaset, backcall, gmplot, pandocfilters, prometheus-client, pyrsistent, tornado, transforms3d\n",
            "  Building wheel for pandaset (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandaset: filename=pandaset-0.2.dev0-py3-none-any.whl size=13428 sha256=093b0e64cce5495c30c2e4f4d5170ac9272eafcb9cc94bce125ec5e5b1d53752\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dhlryxyp/wheels/2c/ae/0e/0fef1c7c1f45c462b26a048a8b556d65fde04b33b371c5e72a\n",
            "  Building wheel for backcall (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for backcall: filename=backcall-0.1.0-py3-none-any.whl size=10413 sha256=eecc20f94e59a57c99e6f4ccf88c917112a8fe513e4d415cb72d0f45b9f2471f\n",
            "  Stored in directory: /root/.cache/pip/wheels/60/5a/10/2177abb11261d49069a732cbc0e66207783c7ee79c1f807167\n",
            "  Building wheel for gmplot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gmplot: filename=gmplot-1.2.0-py3-none-any.whl size=143762 sha256=ccc64a96c353291fb2229a5caa83c308a164a7aa715de32cf30ab269b1c90497\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/bb/61/143790d9935e333669dd6aeef38f7aec9b3a1b7007f47d6fe8\n",
            "  Building wheel for pandocfilters (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandocfilters: filename=pandocfilters-1.4.2-py3-none-any.whl size=7871 sha256=a6ffc1171020c5951a15dc75f3926eccfc81a5371161ac840f92840f64c4e89a\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/99/01/9fe785b86d1e091a6b2a61e06ddb3d8eb1bc9acae5933d4740\n",
            "  Building wheel for prometheus-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prometheus-client: filename=prometheus_client-0.7.1-py3-none-any.whl size=41404 sha256=2d66fbcfdd8d58a074f46081dba6f3d282c9e3434e98cdced7d51795f21213a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/0c/26/59ba285bf65dc79d195e9b25e2ddde4c61070422729b0cd914\n",
            "  Building wheel for pyrsistent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrsistent: filename=pyrsistent-0.16.0-cp37-cp37m-linux_x86_64.whl size=98781 sha256=364ffc0d59cea4278fbe26f150d67df117e17b9b55ece52bcde419bcde3abb0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/52/11/f0920f95c23ed7d2d0b05f2b7b2f4509e87a20cfe8ea43d987\n",
            "  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tornado: filename=tornado-6.0.4-cp37-cp37m-linux_x86_64.whl size=428602 sha256=4a514026ee8518ebc91c666de988c094c47ab29c769a5acfb7e3759b38cc759d\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/14/fa/d88fb5da77d813ea0ffca38a2ab2a052874e9e1142bad0b348\n",
            "  Building wheel for transforms3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transforms3d: filename=transforms3d-0.3.1-py3-none-any.whl size=59373 sha256=f31f9ec9ac7a6dd3a2730d01edf39cd147a461045dd3233997edc8e4d42224d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b7/93/8985551f83720ce37548a5b543c75380bb707955a9c2c5d28c\n",
            "Successfully built pandaset backcall gmplot pandocfilters prometheus-client pyrsistent tornado transforms3d\n",
            "Installing collected packages: zipp, six, wcwidth, traitlets, pyrsistent, ptyprocess, parso, importlib-metadata, attrs, tornado, pyzmq, python-dateutil, prompt-toolkit, MarkupSafe, jupyter-core, jsonschema, jedi, backcall, testpath, pandocfilters, nbformat, jupyter-client, Jinja2, ipython, defusedxml, bleach, terminado, Send2Trash, prometheus-client, ipykernel, notebook, widgetsnbextension, urllib3, QtPy, idna, certifi, qtconsole, pytz, numpy, jupyter-console, ipywidgets, transforms3d, Pillow, pandas, gmplot, appnope, pandaset\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.6.0\n",
            "    Uninstalling zipp-3.6.0:\n",
            "      Successfully uninstalled zipp-3.6.0\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.2.5\n",
            "    Uninstalling wcwidth-0.2.5:\n",
            "      Successfully uninstalled wcwidth-0.2.5\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.1.1\n",
            "    Uninstalling traitlets-5.1.1:\n",
            "      Successfully uninstalled traitlets-5.1.1\n",
            "  Attempting uninstall: pyrsistent\n",
            "    Found existing installation: pyrsistent 0.18.0\n",
            "    Uninstalling pyrsistent-0.18.0:\n",
            "      Successfully uninstalled pyrsistent-0.18.0\n",
            "  Attempting uninstall: ptyprocess\n",
            "    Found existing installation: ptyprocess 0.7.0\n",
            "    Uninstalling ptyprocess-0.7.0:\n",
            "      Successfully uninstalled ptyprocess-0.7.0\n",
            "  Attempting uninstall: parso\n",
            "    Found existing installation: parso 0.8.2\n",
            "    Uninstalling parso-0.8.2:\n",
            "      Successfully uninstalled parso-0.8.2\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.8.2\n",
            "    Uninstalling importlib-metadata-4.8.2:\n",
            "      Successfully uninstalled importlib-metadata-4.8.2\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 21.2.0\n",
            "    Uninstalling attrs-21.2.0:\n",
            "      Successfully uninstalled attrs-21.2.0\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: pyzmq\n",
            "    Found existing installation: pyzmq 22.3.0\n",
            "    Uninstalling pyzmq-22.3.0:\n",
            "      Successfully uninstalled pyzmq-22.3.0\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Uninstalling MarkupSafe-2.0.1:\n",
            "      Successfully uninstalled MarkupSafe-2.0.1\n",
            "  Attempting uninstall: jupyter-core\n",
            "    Found existing installation: jupyter-core 4.9.1\n",
            "    Uninstalling jupyter-core-4.9.1:\n",
            "      Successfully uninstalled jupyter-core-4.9.1\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "  Attempting uninstall: jedi\n",
            "    Found existing installation: jedi 0.18.1\n",
            "    Uninstalling jedi-0.18.1:\n",
            "      Successfully uninstalled jedi-0.18.1\n",
            "  Attempting uninstall: backcall\n",
            "    Found existing installation: backcall 0.2.0\n",
            "    Uninstalling backcall-0.2.0:\n",
            "      Successfully uninstalled backcall-0.2.0\n",
            "  Attempting uninstall: testpath\n",
            "    Found existing installation: testpath 0.5.0\n",
            "    Uninstalling testpath-0.5.0:\n",
            "      Successfully uninstalled testpath-0.5.0\n",
            "  Attempting uninstall: pandocfilters\n",
            "    Found existing installation: pandocfilters 1.5.0\n",
            "    Uninstalling pandocfilters-1.5.0:\n",
            "      Successfully uninstalled pandocfilters-1.5.0\n",
            "  Attempting uninstall: nbformat\n",
            "    Found existing installation: nbformat 5.1.3\n",
            "    Uninstalling nbformat-5.1.3:\n",
            "      Successfully uninstalled nbformat-5.1.3\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 5.3.5\n",
            "    Uninstalling jupyter-client-5.3.5:\n",
            "      Successfully uninstalled jupyter-client-5.3.5\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: defusedxml\n",
            "    Found existing installation: defusedxml 0.7.1\n",
            "    Uninstalling defusedxml-0.7.1:\n",
            "      Successfully uninstalled defusedxml-0.7.1\n",
            "  Attempting uninstall: bleach\n",
            "    Found existing installation: bleach 4.1.0\n",
            "    Uninstalling bleach-4.1.0:\n",
            "      Successfully uninstalled bleach-4.1.0\n",
            "  Attempting uninstall: terminado\n",
            "    Found existing installation: terminado 0.12.1\n",
            "    Uninstalling terminado-0.12.1:\n",
            "      Successfully uninstalled terminado-0.12.1\n",
            "  Attempting uninstall: Send2Trash\n",
            "    Found existing installation: Send2Trash 1.8.0\n",
            "    Uninstalling Send2Trash-1.8.0:\n",
            "      Successfully uninstalled Send2Trash-1.8.0\n",
            "  Attempting uninstall: prometheus-client\n",
            "    Found existing installation: prometheus-client 0.12.0\n",
            "    Uninstalling prometheus-client-0.12.0:\n",
            "      Successfully uninstalled prometheus-client-0.12.0\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "  Attempting uninstall: notebook\n",
            "    Found existing installation: notebook 5.3.1\n",
            "    Uninstalling notebook-5.3.1:\n",
            "      Successfully uninstalled notebook-5.3.1\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.5.2\n",
            "    Uninstalling widgetsnbextension-3.5.2:\n",
            "      Successfully uninstalled widgetsnbextension-3.5.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: QtPy\n",
            "    Found existing installation: QtPy 1.11.2\n",
            "    Uninstalling QtPy-1.11.2:\n",
            "      Successfully uninstalled QtPy-1.11.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2021.10.8\n",
            "    Uninstalling certifi-2021.10.8:\n",
            "      Successfully uninstalled certifi-2021.10.8\n",
            "  Attempting uninstall: qtconsole\n",
            "    Found existing installation: qtconsole 5.2.0\n",
            "    Uninstalling qtconsole-5.2.0:\n",
            "      Successfully uninstalled qtconsole-5.2.0\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: jupyter-console\n",
            "    Found existing installation: jupyter-console 5.2.0\n",
            "    Uninstalling jupyter-console-5.2.0:\n",
            "      Successfully uninstalled jupyter-console-5.2.0\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.6.5\n",
            "    Uninstalling ipywidgets-7.6.5:\n",
            "      Successfully uninstalled ipywidgets-7.6.5\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nbclient 0.5.9 requires jupyter-client>=6.1.5, but you have jupyter-client 6.1.2 which is incompatible.\n",
            "markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 1.5.2 which is incompatible.\n",
            "kapre 0.3.6 requires numpy>=1.18.5, but you have numpy 1.18.2 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 5.2.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.13.0 which is incompatible.\n",
            "google-colab 1.0.0 requires notebook~=5.3.0; python_version >= \"3.0\", but you have notebook 6.0.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.0.3 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.14.0 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.0.4 which is incompatible.\n",
            "fbprophet 0.7.1 requires pandas>=1.0.4, but you have pandas 1.0.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "bokeh 2.3.3 requires pillow>=7.1.0, but you have pillow 7.0.0 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Jinja2-2.11.1 MarkupSafe-1.1.1 Pillow-7.0.0 QtPy-1.9.0 Send2Trash-1.5.0 appnope-0.1.0 attrs-19.3.0 backcall-0.1.0 bleach-3.1.4 certifi-2019.11.28 defusedxml-0.6.0 gmplot-1.2.0 idna-2.9 importlib-metadata-1.5.2 ipykernel-5.2.0 ipython-7.13.0 ipywidgets-7.5.1 jedi-0.16.0 jsonschema-3.2.0 jupyter-client-6.1.2 jupyter-console-6.1.0 jupyter-core-4.6.3 nbformat-5.0.4 notebook-6.0.3 numpy-1.18.2 pandas-1.0.3 pandaset-0.2.dev0 pandocfilters-1.4.2 parso-0.6.2 prometheus-client-0.7.1 prompt-toolkit-3.0.5 ptyprocess-0.6.0 pyrsistent-0.16.0 python-dateutil-2.8.1 pytz-2019.3 pyzmq-19.0.0 qtconsole-4.7.2 six-1.14.0 terminado-0.8.3 testpath-0.4.4 tornado-6.0.4 traitlets-4.3.3 transforms3d-0.3.1 urllib3-1.25.8 wcwidth-0.1.9 widgetsnbextension-3.5.1 zipp-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "PIL",
                  "dateutil",
                  "ipykernel",
                  "ipywidgets",
                  "jupyter_client",
                  "jupyter_core",
                  "numpy",
                  "pandas",
                  "prompt_toolkit",
                  "pytz",
                  "six",
                  "tornado",
                  "traitlets",
                  "wcwidth",
                  "zmq"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdHS35bKy0S-",
        "outputId": "a7dc6e10-85e3-4228-80c6-78abed3c10a4"
      },
      "source": [
        "from pandaset import DataSet\n",
        "\n",
        "dataset = DataSet('/content/drive/My Drive/Truckin/')\n",
        "\n",
        "print(dataset)\n",
        "\n",
        "print(\"The sequences that have (cuboid + semantic) segmentation annotations:\\n\", dataset.sequences(with_semseg=True))\n",
        "\n",
        "for i in dataset.sequences(with_semseg=False):\n",
        "    print(i)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pandaset.dataset.DataSet object at 0x7fbefd84e350>\n",
            "The sequences that have (cuboid + semantic) segmentation annotations:\n",
            " ['005', '002', '003', '001']\n",
            "005\n",
            "006\n",
            "002\n",
            "004\n",
            "003\n",
            "008\n",
            "001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0BZKkhoExQU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_udgoMGPziAt"
      },
      "source": [
        "Move Data from drive to dataset object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6rSet2UzhNX"
      },
      "source": [
        "seq002 = dataset['002']\n",
        "\n",
        "#load into memory\n",
        "seq002.load()\n",
        "\n",
        "print(seq002.__dict__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDrUXLU77pQa"
      },
      "source": [
        "Now that we have seen how the seq and data set look like we need to access the lidar data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tb2hd5r8QUF"
      },
      "source": [
        "lidarObj = seq002.lidar\n",
        "\n",
        "print(lidarObj)\n",
        "\n",
        "print(f\"Time stamps from lidar object: {lidarObj.timestamps}\")\n",
        "\n",
        "#print(f\"Panda set object: {pandaset.sensors.Lidar}\")\n",
        "\n",
        "print(f\"Lidar data obj {lidarObj.data}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRDJftFqHHer"
      },
      "source": [
        "Lidar to Pytorch Var"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh44GfP0HNf-"
      },
      "source": [
        "import plotly.graph_objects as go \n",
        "  \n",
        "headerColor = 'rgb(116,0,4)'\n",
        "\n",
        "annon_class = []\n",
        "for key in seq002.semseg.classes:\n",
        "    annon_class.append(seq002.semseg.classes[key])\n",
        "  \n",
        "fig = go.Figure(data=[go.Table( \n",
        "    header = dict(values=['<b>Semantic Segmentation Annotations for Sequence 002 (Possible Labels)</b>'],\n",
        "                  line_color='darkslategray',\n",
        "                  fill_color=headerColor,\n",
        "                  align=['left','center'],\n",
        "                  font=dict(color='white', size=12)\n",
        "                 ), \n",
        "    cells = dict(values=[annon_class],\n",
        "                 line_color='darkslategray',\n",
        "                 align = ['left', 'center'],\n",
        "                 font = dict(color = headerColor, size = 11)\n",
        "                )) \n",
        "                     ]) \n",
        "fig.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klhC4vJ-HUUa"
      },
      "source": [
        "lidar_poses = seq002.lidar._poses\n",
        "print(lidar_poses[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPrCIjh2HY_C"
      },
      "source": [
        "import csv\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pandaset\n",
        "\n",
        "import os\n",
        "   \n",
        "pfile = \"/content/drive/My Drive/Truckin/001/lidar/01.pkl\"\n",
        "with open(pfile, \"rb\") as fin:\n",
        "    data = pickle.load(fin)\n",
        "\n",
        "data.info()\n",
        "\n",
        "dataset = pandaset.DataSet(\"/content/drive/My Drive/Truckin/\")\n",
        "seq002 = dataset[\"002\"]\n",
        "seq002.load_lidar().load_semseg()\n",
        "\n",
        "points3d_lidar_xyz = seq002.lidar.data\n",
        "\n",
        "print(seq002.semseg.classes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsEjIqHaHkpb"
      },
      "source": [
        "3d plots of lidar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoXmyiwnHkCZ"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "import random\n",
        "\n",
        "ax = plt.axes(projection='3d')\n",
        "\n",
        "xyz = data[['x', 'y', 'z']].to_numpy()\n",
        "color_maps = [(random.random(), random.random(), random.random()) for _ in range(1000 + 1)]\n",
        "ax.scatter(xyz[:,0], xyz[:,1], xyz[:,2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXkO6uKZH112"
      },
      "source": [
        "Lidar Data to pytorch var"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpMV_QHSIzhI"
      },
      "source": [
        "!pip install torch\n",
        "!echo torch version "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRnuagyRH8Z-"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "xyz = data[['x', 'y', 'z']].to_numpy()\n",
        "print(xyz)\n",
        "print(xyz.shape)\n",
        "\n",
        "\n",
        "xyz_torch = torch.from_numpy(xyz)\n",
        "print(xyz_torch)\n",
        "print(type(xyz_torch))\n",
        "print(xyz_torch.size())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVNBzrVfH49j"
      },
      "source": [
        "Goal: Train model one frame at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoGAjbRGO_JJ"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "lidarList = ['/00.pkl', '/01.pkl','/02.pkl','/03.pkl','/04.pkl','/05.pkl','/06.pkl','/07.pkl', \n",
        "             '/08.pkl','/09.pkl','/10.pkl','/11.pkl', '/11.pkl','/12.pkl','/13.pkl','/14.pkl',\n",
        "             '/15.pkl','/16.pkl','/17.pkl','/18.pkl','/19.pkl','/20.pkl','/21.pkl','/22.pkl',\n",
        "             '/23.pkl','/24.pkl', '/25.pkl','/26.pkl','/27.pkl','/28.pkl','/29.pkl', \n",
        "             '/30.pkl', '/31.pkl','/32.pkl','/33.pkl','/34.pkl','/35.pkl','/36.pkl','/37.pkl', '/38.pkl','/39.pkl',\n",
        "             '/40.pkl', '/41.pkl','/42.pkl','/43.pkl','/44.pkl','/45.pkl','/46.pkl','/47.pkl', '/48.pkl','/49.pkl',\n",
        "             '/50.pkl', '/51.pkl','/52.pkl','/53.pkl','/54.pkl','/55.pkl','/56.pkl','/57.pkl', '/58.pkl','/59.pkl',\n",
        "             '/60.pkl', '/61.pkl','/62.pkl','/63.pkl','/64.pkl','/65.pkl','/66.pkl','/67.pkl', '/68.pkl','/69.pkl',\n",
        "             '/70.pkl', '/71.pkl','/72.pkl','/73.pkl','/74.pkl','/75.pkl','/76.pkl','/77.pkl', '/78.pkl','/79.pkl']\n",
        "\n",
        "for lidarFile in lidarList:\n",
        "    pfile = \"/content/drive/My Drive/Truckin/001/lidar\" + lidarFile\n",
        "    with open(pfile, \"rb\") as fin:\n",
        "        data = pickle.load(fin)\n",
        "\n",
        "    data.info()\n",
        "\n",
        "    xyz = data[['x', 'y', 'z']].to_numpy()\n",
        "    print(xyz)\n",
        "    print(xyz.shape)\n",
        "\n",
        "\n",
        "    xyz_torch = torch.from_numpy(xyz)\n",
        "    print(xyz_torch)\n",
        "    print(type(xyz_torch))\n",
        "    print(xyz_torch.size())\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX5dsnXUadqw"
      },
      "source": [
        "Get labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgudlOJdafNB"
      },
      "source": [
        "pfile = \"/content/drive/MyDrive/Truckin/002/annotations/cuboids/01.pkl\"\n",
        "with open(pfile, \"rb\") as fin:\n",
        "    data = pickle.load(fin)\n",
        "\n",
        "data.info() #Get data dictionary\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "annotationList = ['/00.pkl', '/01.pkl','/02.pkl','/03.pkl','/04.pkl','/05.pkl','/06.pkl','/07.pkl', \n",
        "             '/08.pkl','/09.pkl','/10.pkl','/11.pkl', '/11.pkl','/12.pkl','/13.pkl','/14.pkl',\n",
        "             '/15.pkl','/16.pkl','/17.pkl','/18.pkl','/19.pkl','/20.pkl','/21.pkl','/22.pkl',\n",
        "             '/23.pkl','/24.pkl', '/25.pkl','/26.pkl','/27.pkl','/28.pkl','/29.pkl', \n",
        "             '/30.pkl', '/31.pkl','/32.pkl','/33.pkl','/34.pkl','/35.pkl','/36.pkl','/37.pkl', '/38.pkl','/39.pkl',\n",
        "             '/40.pkl', '/41.pkl','/42.pkl','/43.pkl','/44.pkl','/45.pkl','/46.pkl','/47.pkl', '/48.pkl','/49.pkl',\n",
        "             '/50.pkl', '/51.pkl','/52.pkl','/53.pkl','/54.pkl','/55.pkl','/56.pkl','/57.pkl', '/58.pkl','/59.pkl',\n",
        "             '/60.pkl', '/61.pkl','/62.pkl','/63.pkl','/64.pkl','/65.pkl','/66.pkl','/67.pkl', '/68.pkl','/69.pkl',\n",
        "             '/70.pkl', '/71.pkl','/72.pkl','/73.pkl','/74.pkl','/75.pkl','/76.pkl','/77.pkl', '/78.pkl','/79.pkl']\n",
        "\n",
        "for annotationFile in annotationList:\n",
        "    pfile = \"/content/drive/MyDrive/Truckin/001/annotations/cuboids\" + annotationFile\n",
        "    with open(pfile, \"rb\") as fin:\n",
        "        data = pickle.load(fin)\n",
        "\n",
        "    #data.info()\n",
        "\n",
        "    #print(data.label)\n",
        "\n",
        "    print(f\"X: {data['position.x']}, Y: {data['position.y']}, Z: {data['position.z']}, Label: {data.label}\") \n",
        "    \n",
        "    \"\"\"xyz = data[['x', 'y', 'z']].to_numpy()\n",
        "    print(xyz)\n",
        "    print(xyz.shape)\n",
        "\n",
        "\n",
        "    xyz_torch = torch.from_numpy(xyz)\n",
        "    print(xyz_torch)\n",
        "    print(type(xyz_torch))\n",
        "    print(xyz_torch.size())\"\"\"\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55_4DqQhPTbx"
      },
      "source": [
        "PCA to scale dimension from X , 3 to 150,000 , 3 for uniformity among the different times and scenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TUVDxKrUaYB"
      },
      "source": [
        "print(xyz_torch)\n",
        "print(xyz_torch.size())\n",
        "\n",
        "def PCA_svd(X, k, center=True):\n",
        "  n = X.size()[0]\n",
        "  ones = torch.ones(n).view([n,1])\n",
        "  h = ((1/n) * torch.mm(ones, ones.t())) if center  else torch.zeros(n*n).view([n,n])\n",
        "  H = torch.eye(n) - h\n",
        "  X_center =  torch.mm(H.double(), X.double())\n",
        "  u, s, v = torch.svd(X_center) \n",
        "  components  = v[:k].t()\n",
        "  explained_variance = torch.mul(s[:k], s[:k])/(n-1)\n",
        "  return { 'X':X, 'k':k, 'components':components,     \n",
        "    'explained_variance':explained_variance }\n",
        "\n",
        "value = PCA_svd(xyz_torch, 3)\n",
        "print(value)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_8se6QzBU3U"
      },
      "source": [
        "Camera Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUwaq3NFBXBx"
      },
      "source": [
        "from pandaset import DataSet\n",
        "\n",
        "dataset = DataSet('/content/drive/My Drive/Truckin/')\n",
        "\n",
        "seq002 = dataset['002']\n",
        "\n",
        "seq002.load()\n",
        "\n",
        "print(seq002.__dict__)\n",
        "\n",
        "cameraObj = seq002.camera\n",
        "\n",
        "print(cameraObj)\n",
        "\n",
        "#print(f\"Time stamps from lidar object: {cameraObj.timestamps}\")\n",
        "\n",
        "#print(f\"Panda set object: {pandaset.sensors.Lidar}\")\n",
        "\n",
        "#print(f\"Lidar data obj {cameraObj.data}\")\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import pickle\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "cameraList = ['/00.jpg', '/01.jpg','/02.jpg','/03.jpg','/04.jpg','/05.jpg','/06.jpg','/07.jpg', \n",
        "             '/08.jpg','/09.jpg','/10.jpg','/11.jpg', '/11.jpg','/12.jpg','/13.jpg','/14.jpg',\n",
        "             '/15.jpg','/16.jpg','/17.jpg','/18.jpg','/19.jpg','/20.jpg','/21.jpg','/22.jpg',\n",
        "             '/23.jpg','/24.jpg', '/25.jpg','/26.jpg','/27.jpg','/28.jpg','/29.jpg', \n",
        "             '/30.jpg', '/31.jpg','/32.jpg','/33.jpg','/34.jpg','/35.jpg','/36.jpg','/37.jpg', '/38.jpg','/39.jpg',\n",
        "             '/40.jpg', '/41.jpg','/42.jpg','/43.jpg','/44.jpg','/45.jpg','/46.jpg','/47.jpg', '/48.jpg','/49.jpg',\n",
        "             '/50.jpg', '/51.jpg','/52.jpg','/53.jpg','/54.jpg','/55.jpg','/56.jpg','/57.jpg', '/58.jpg','/59.jpg',\n",
        "             '/60.jpg', '/61.jpg','/62.jpg','/63.jpg','/64.jpg','/65.jpg','/66.jpg','/67.jpg', '/68.jpg','/69.jpg',\n",
        "             '/70.jpg', '/71.jpg','/72.jpg','/73.jpg','/74.jpg','/75.jpg','/76.jpg','/77.jpg', '/78.jpg','/79.jpg']\n",
        "\n",
        "for cameraFile in cameraList:\n",
        "    cfile = \"/content/drive/MyDrive/Truckin/001/camera/front_camera\" + cameraFile\n",
        "    #with open(pfile, \"rb\") as fin:\n",
        "    img = Image.open(cfile)\n",
        "\n",
        "    #data.info()\n",
        "\n",
        "    convert_tensor = transforms.ToTensor()\n",
        "\n",
        "    img_tensor = convert_tensor(img)\n",
        "\n",
        "    print(img_tensor)\n",
        "    #xyz = data[['x', 'y', 'z']].to_numpy()\n",
        "    #print(xyz)\n",
        "    print(f\"Shape: {img_tensor.shape}\")\n",
        "\n",
        "\n",
        "    #xyz_torch = torch.from_numpy(xyz)\n",
        "    #print(xyz_torch)\n",
        "    #print(type(xyz_torch))\n",
        "    #print(xyz_torch.size())\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IGa0ChBY3ma"
      },
      "source": [
        "Lidar Only CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W1i5BwllJNI"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.init import xavier_uniform_, zeros_, kaiming_normal_\n",
        "\n",
        "\n",
        "class PointNetfeat(nn.Module):\n",
        "    def __init__(self, pts_dim, x=1):\n",
        "        super(PointNetfeat, self).__init__()\n",
        "        self.output_channel = 512 * x\n",
        "        self.conv1 = torch.nn.Conv1d(pts_dim, 64 * x, 1)\n",
        "        self.conv2 = torch.nn.Conv1d(64 * x, 128 * x, 1)\n",
        "        self.conv3 = torch.nn.Conv1d(128 * x, self.output_channel, 1)\n",
        "        self.bn1 = nn.BatchNorm1d(64 * x)\n",
        "        self.bn2 = nn.BatchNorm1d(128 * x)\n",
        "        self.bn3 = nn.BatchNorm1d(self.output_channel)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.bn3(self.conv3(x)) # NOTE: should not put a relu\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, self.output_channel)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PointNet(nn.Module):\n",
        "    def __init__(self, pts_dim, x, CLS_NUM):\n",
        "        super(PointNet, self).__init__()\n",
        "        self.feat = PointNetfeat(pts_dim, x)\n",
        "        self.fc1 = nn.Linear(512 * x, 256 * x)\n",
        "        self.fc2 = nn.Linear(256 * x, 256)\n",
        "\n",
        "        self.pre_bn = nn.BatchNorm1d(pts_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(256 * x)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.relu = nn.ReLU()\n",
        "        # NOTE: should there put a BN?\n",
        "        self.fc_s1 = nn.Linear(256, 256)\n",
        "        self.fc_s2 = nn.Linear(256, 3, bias=False)\n",
        "        self.fc_c1 = nn.Linear(256, 256)\n",
        "        self.fc_c2 = nn.Linear(256, CLS_NUM, bias=False)\n",
        "        self.fc_ce1 = nn.Linear(256, 256)\n",
        "        self.fc_ce2 = nn.Linear(256, 3, bias=False)\n",
        "        self.fc_hr1 = nn.Linear(256, 256)\n",
        "        self.fc_hr2 = nn.Linear(256, 1, bias=False)\n",
        "\n",
        "    def forward(self, x, pred_bbox):\n",
        "        x = self.feat(self.pre_bn(x))\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        feat = F.relu(self.bn2(self.fc2(x)))\n",
        "\n",
        "        x = F.relu(self.fc_c1(feat))\n",
        "        logits = self.fc_c2(x)\n",
        "\n",
        "        x = F.relu(self.fc_ce1(feat))\n",
        "        centers = self.fc_ce2(x)\n",
        "\n",
        "        x = F.relu(self.fc_s1(feat))\n",
        "        sizes = self.fc_s2(x)\n",
        "\n",
        "        x = F.relu(self.fc_hr1(feat))\n",
        "        headings = self.fc_hr2(x)\n",
        "\n",
        "        return logits, centers, sizes, headings\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
        "                kaiming_normal_(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    zeros_(m.bias)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPXJ4w7yvsBk"
      },
      "source": [
        "Image CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsBhUG9swHVf"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "\n",
        "#Loss and Optimizer\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxRVIgmGZhsY"
      },
      "source": [
        "Both CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsBDugAWZgY7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}